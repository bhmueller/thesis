\section{Introduction} \label{intro}

Economics is thinking in models that abstract from the actually existing economic systems by imposing assumptions on economic relations. Those assumptions are defendable as long as they are sufficiently realistic for the research question at hand \citep{F66}, what prompts the question, what sufficiently realistic means. Often, assumptions are decided upon by mathematical convenience: do the assumptions grant that the model is solvable, i.e. whether a solution exists in closed form and whether this solution is unique.

Due to the advancements in computational tractability one is less coerced to impose insufficiently realistic assumptions if numerical methods can be applied. That is, since even if the model at hand does not have an analytical solution, one can still obtain an approximate solution by applying numerical methods \citep{MF04}.

[Example with Rust model and why no analytical solution exists.]

\noindent Consider the following model
\begin{equation*}
Y = f(X),
\end{equation*}

where $Y$ is the response variable that depends on the values of some independent variables $X$. Let $X$ denote the vector $X = (X_1, \dots, X_k)' \in \mathbb{R}^k$, where $k$ is the number of independent variables. Let $x$ be the vector of specific values assigned to $X$, i.e. one realisation of $X$. Let $f(\cdot)$ denote a function that describes in which way $Y$ depends on $X$. This function may be some complex function (e.g. computer code) or a model that can be solved by numerical methods only. Thus, $f$ is generally not available in closed form and, hence, the relationship between $Y$ and $X$ is considered a black box.

In the remainder of this work, I name $Y$ the (model) output and the vector $X$ the (model) inputs. In this work I will consider $X$ as being stochastic, that is, the $X_i$, $i = 1,\dots, k$, follow a joint cumulative distribution function $G(X)$. Thus, although $f$ is assumed to be a deterministic function of $X$, the output $Y$ is also stochastic due to the uncertainty in $X$ \citep{SNS16}.

In order to understand the model behaviour, sensitivity analysis is a way of learning about the input-output relationship of the model at hand \citep{BP16}. One can generally structure sensitivity analysis into local and global methods of sensitivity analysis \citep{BP16}. Local methods conduct sensitivity analysis around a certain point, or base case, $x_0$, in a deterministic framework, i.e. no probability distribution is assigned to $X$ \citep{BP16}. In contrast, when performing global sensitivity analysis, we find ourselves in a stochastic context, which requires knowledge of the distribution of $X$, be it joint or marginal with or without dependence between the inputs \citep{ST02}. The settings for which sensitivity analysis methods can yield useful information are further discussed in \cref{var_based_sa}.

The result of performing sensitivity analysis is some sensitivity measure, that depends on the sensitivity analysis method we apply to the context we find ourselves in. In this work, I assess the predictive validity of Morris indices, a qualitative sensitivity method \citep{M91}. Qualitative sensitivity methods aim at i. identifying uninfluential inputs and ii. ranking inputs with respect to their importance \citep{BP16}. I compare the performance of the Morris method for dependent inputs as proposed by \citet{GM17} to Shapley effects, a variance-based quantitative sensitivity method \citep{O14}.

Variance-based sensitivity analysis, a method that evaluates the importance of an input by assigning to it the expected reduction in output variance if this input was known with certainty \citep{BP16}. Shapley values are a concept from game theory introduced by \citet{S53}. The main advantages of Shapley values for sensitivity analysis are that they are as easily applied to dependent inputs as they are interpreted. Furthermore, Shapley values satisfy a range of desirable properties, the most important being efficiency and the null-player property \citet{S53, O14}. In the context of sensitivity analysis, \citet{SNS16} use the term Shapley effects. Despite of these appealing features, estimation of Shapley effects can be computationally demanding \citep{SNS16}.

This is where the Morris method comes into play, since it promises to serve similar purposes as Shapley effects, but come at a lower computational cost. In this work I estimate Morris indices for a classical structural econometric model, the single-agent dynamic stochastic model of discrete choice introduced by \citet{R87} and compare these to Shapley effects. Interestingly, I find that the Morris method is a worthy substitute for Shapley effects since they perform very well in identifying uninfluential inputs and in ranking the inputs accordingly for the Rust model.

% One set of common variance-based sensitivity measure are the total and first-order effects, or Sobol' indices, introduced by \citet{S93} for independently distributed input variables, which are based on an Analysis of Variances (henceforth ANOVA) decomposition. One obvious drawback of the Sobol' indices in the original definition is that they rely on the assumption of inputs independence. Even if the ANOVA decomposition is adapted to handle dependent inputs, they suffer from problems \citep{OP17} which are discussed in \cref{var_based_sa}. \citet{OP17} suggest using Shapley values instead, which were suggested in the context