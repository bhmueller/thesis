\section{Introduction} \label{intro}

Economics is thinking in models that abstract from the actually existing economic systems by imposing assumptions on economic relations. Those assumptions are defendable as long as they are sufficiently realistic for the research question at hand \citep{F66}, what prompts the question, what sufficiently realistic means. Often, assumptions are decided upon by mathematical convenience: do the assumptions grant that the model is solvable, i.e. whether a solution exists in closed form and whether this solution is unique.

Due to the advancements in computational tractability we are less coerced to impose insufficiently realistic assumptions if numerical methods can be applied. That is, since even if the model at hand does not have an analytical solution, one can still obtain an approximate solution by applying numerical methods \citep{MF04}.

[Example with Rust model and why no analytical solution exists.]

\noindent Consider the following model
\begin{equation*}
Y = f(X),
\end{equation*}

where $Y$ is the response variable that depends on the values of some independent variables $X$. Let $X$ denote the vector $X = (X_1, \dots, X_k)' \in \mathbb{R}^k$, where $k$ is the number of independent variables. Let $x$ be the vector of specific values assigned to $X$, i.e. one realisation of $X$. Let $f(\cdot)$ denote a function that describes in which way $Y$ depends on $X$. This function may be some complex function (e.g. computer code) or a model that can be solved by numerical methods only. Thus, $f$ is generally not available in closed form and, hence, the relationship between $Y$ and $X$ is considered a black box.

In the remainder of this work, I name $Y$ the (model) output and the vector $X$ the (model) inputs. In this work I will consider $X$ as being stochastic, that is, the $X_i$, $i = 1,\dots, k$, follow a joint cumulative distribution function $G(X)$. Thus, although $f$ is assumed to be a deterministic function of $X$, the output $Y$ is also stochastic due to the uncertainty in $X$ \citep{SNS16}. Sensitivity analysis sheds light on this input-output relationship \citep{BP16}.

In the context of structural econometrics sensitivity analysis has an important role. Structural econometrics imposes structure on the relationship between economic parameters \citep{LM17}. The structural model is then taken to the data and deep economic parameters are estimated \citep{LM17}. Structural econometrics can be used to investigate counterfactual policies by quantifying the effects of a policy on some output \citep{LM17}. If these models are actually used for policy making, the structural model and its assumptions have real-world consequences. The question arises whether the imposed model assumptions are realistic enough to inform policy. Therefore, thorough modelling is crucial for structural econometrics.

Sensitivity analysis analyses how sensitive the model outcome is to model inputs \citep{R21}. Any assumption we impose can be such an input \citep{R21}. For example, an estimated parameter we assume to be deterministic by assigning a value to it, we implicitly neglect that the estimate is subject to uncertainty \citep{R21}. Making this assumption can be problematic if the model output is very sensitive to the value assigned to this input. Such an input we call ``important". Sensitivity analysis can guide research by identifying these important inputs \citep{R21}. For example, we could put more effort into estimating such important inputs more precisely, thus reducing output uncertainty.

In order to understand the model behaviour, sensitivity analysis is a way of learning about the input-output relationship of the model at hand \citep{BP16}. One can generally structure sensitivity analysis into local and global methods of sensitivity analysis \citep{BP16}. Local methods conduct sensitivity analysis around a certain point, or base case, $x_0$, in a deterministic framework, i.e. no probability distribution is assigned to $X$ \citep{BP16}. In contrast, when performing global sensitivity analysis, we find ourselves in a stochastic context, which requires knowledge of the distribution of $X$, be it joint or marginal with or without dependence between the inputs \citep{ST02}.

The result of performing sensitivity analysis is some sensitivity measure, that depends on the sensitivity analysis method we apply to the context we find ourselves in. In this work, I assess the predictive validity of Morris indices, a qualitative sensitivity method introduced by \citet{M91}. Qualitative sensitivity methods aim at identifying uninfluential inputs and ranking inputs with respect to their importance \citep{BP16}. The Morris method employs a one-at-a-time algorithm to assess input importance: it uses the relative change in the model output due to a change in a model input \citep{M91}. I compare the performance of the Morris method for dependent inputs as proposed by \citet{GM17} to Shapley effects, a variance-based quantitative sensitivity method \citep{O14}.

Shapley effects serve as the benchmark to which I compare the input ranking induced by the Morris indices. Shapley effects are becoming more and more popular in the context of sensitivity analysis \citep{PRB20}. Despite of many appealing features, estimation of  Shapley effects can be computationally demanding \citep{SNS16}

% Variance-based sensitivity analysis, a method that evaluates the importance of an input by assigning to it the expected reduction in output variance if this input was known with certainty \citep{BP16}. Shapley values are a concept from game theory introduced by \citet{S53}. The main advantages of Shapley values for sensitivity analysis are that they are as easily applied to dependent inputs as they are interpreted. Furthermore, Shapley values satisfy a range of desirable properties, the most important being efficiency and the null-player property \citet{S53, O14}. In the context of sensitivity analysis, \citet{SNS16} use the term Shapley effects. Despite of these appealing features, estimation of Shapley effects can be computationally demanding \citep{SNS16}.

This is where the Morris method comes into play, since it promises to serve similar purposes as the Shapley effects, but come at a lower computational cost.

% Say something about the literature.



In this work I estimate Morris indices for a classical structural econometric model, the single-agent dynamic stochastic model of discrete choice introduced by \citet{R87} and compare these to Shapley effects. Interestingly, I find that the Morris method is a worthy substitute for Shapley effects since they perform very well in identifying uninfluential inputs and in ranking the inputs accordingly for the Rust model.

% One set of common variance-based sensitivity measure are the total and first-order effects, or Sobol' indices, introduced by \citet{S93} for independently distributed input variables, which are based on an Analysis of Variances (henceforth ANOVA) decomposition. One obvious drawback of the Sobol' indices in the original definition is that they rely on the assumption of inputs independence. Even if the ANOVA decomposition is adapted to handle dependent inputs, they suffer from problems \citep{OP17} which are discussed in \cref{var_based_sa}. \citet{OP17} suggest using Shapley values instead, which were suggested in the context