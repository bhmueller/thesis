\section{The Benchmark: Computation of Shapley Effects} \label{comp_shap}

This section introduces Shapley effects for sa. Shapley effects will serve as a benchmark for evaluating the performance of the sm derived by the Morris method.

\subsection{Shapley Values in Economics}

The Shapley value is a concept from cooperative game theory, introduced by \citet{S53}. It is a value that informs about how to fairly distribute the profit of a product that was created by a team effort. Fairly here means, that any player receives as much of the gain as he or she contributed to the team effort. As individual contributions are not directly observed, \citet{S53} proposes the following value. The Shapley value ensures that every individual receives at least as much as if he or she had if acted independently \citep{IP19}.

Consider a game with $k$ players, where $K$ denotes the set of all players, the grand coalition. A coalition is a subset $J \subset K$. Let $- J$ denote the complement set of $J$, i.e. $- J = K \setminus{J}$. The corresponding game is defined by a coalition value function that maps each coalition $J$ to the value attained by this coalition, i.e. $val: 2^K \to \mathbb{R}_{\geq 0}$, where $2^K$ denotes the power set of $K$ (set of all subsets of $K$) \citep{SNS16}. It is generally assumed that $val(\emptyset) = 0$. The following definition is strongly inspired by the one found in \citet{PRB20}:

\begin{definition}

Given a coalition worth function $val$, the marginal contribution of player $i$ joining coalition $J$ is $mar(J,\ i)=val(J \cup \{i\}) - val(J)$. The Shapley value is then defined by
\begin{equation}
\begin{split}
\phi_{i} (val)& = \sum_{J \mid i \notin J} \frac{\vert J \vert ! (k - \vert J \vert - 1) ! }{ k !} (val(J \cup \{i\}) - val(J)) \\
& = \frac{1}{k} \sum_{J \mid i \notin J} \binom{k-1}{\vert J \vert}^{-1} mar(J,\ i).
\end{split}
\end{equation}

\end{definition}

In words, one considers every coalition of players and evaluates by how much the inclusion of player $i$ increases the overall profit of the coalition. This is achieved by subtracting the value generated by the members of coalition $J$ from the value created by the larger coalition that includes coalition $J$ and player $i$. In the context of a game, the marginal value for each coalition is divided among all members of this coalition, e.g. if the coalition has one member only, the entire marginal value is gained by this player. One can also interpret the Shapley value as the expected payoff from joining a coalition or from leaving the complementary coalition \citep{PRB20}.

\citet{S53} shows that $\phi_i$ satisfies the following four axioms and that the Shapley value is the only value that satisfies all four axioms at the same time. Note that Shapley values normalised by total profit are considered.

\begin{itemize}
    \item Pareto efficiency: $\sum_{i=1}^{k}\phi_i=1$.
	\item Symmetry: If $val(J \cup {i})=val(J \cup {j})\ \forall\ J \subseteq K \setminus{\{i,\ j\}}$, then $\phi_i = \phi_j$.
	\item Linearity: $\phi_i(val_1 + val_2)=\phi_i(val_1)+\phi_i(val_2)$.
	\item Null-player: If $\forall J,\ mar(J,\ i) = 0$ holds then $\phi_i(val) = 0$.
\end{itemize}

Pareto efficiency assures that nothing of the profit gained by the team is wasted. Normalised by the total profit, the Shapley value gives the share attained by each individual in the game. Symmetry results into all players who contributed the same to the common product, receiving the same payoff, reflecting the meritocratic principle. The null-player axiom ensures that players that do not contribute to the team effort at all, do not receive any share in the profits. Especially axioms 1 and 4 are desirable in the context of the Shapley value as a variance-based sm.

\subsection{Variance-based Sensitivity Analysis} \label{var_based_sa}

There are several purposes for which one can apply sa methods in general. Note, that in sa one refers to input variables by the term factors \citep{R21}. In their textbook, \citet{STC04} summarise the following four possible objectives of sa.

Factors Prioritisation (FP): In the FP setting, one determines on which inputs applied, uncertainty reduction results into the largest reduction in output uncertainty. SA determines the importance of an input variable. Applied to all inputs, one can derive a ranking of all inputs in order of importance. FP can guide research by prioritising inputs. Inputs are identified for which better experimental measurement can reduce output uncertainty the most, supposing that additional information costs the same for alle inputs.

Factors Fixing (FF): The FF setting allows one to determine the least influential inputs. These then can be fixed at a specific value without losing information in the model output. FF can also be seen as inputs screening. One would want to fix the least influential inputs to reduce dimensionality of the model and thus the complexity and computational burden.

Variance Cutting (VC): VC informs about which inputs to fix to arrive at a certain desired value of the output variance, under the condition that the smallest number of inputs are fixed. VC is most useful for risk assessment.

Factors Mapping (FM): If one cares about the input importance in certain regions of output values only, one can apply FM. First, one classifies output values into groups and only then employs the importance exercise. So, the inputs are determined that contribute the most to producing output values in a target region. E.g. one could be interested in a certain percentile of the output range. The term mapping stems from mapping the importance of the inputs to the categories of $Y$.

SA does address more fundamental objectives of mathematical modelling and the analysis of systems that can be achieved directly from the above settings and are summarised in \citet{R21}. By sa, one can achieve or conduct a. scientific discovery, i.e. identification of causal relationships, b. dimensionality reduction, i.e. determination of the least important inputs, which is achieved by FF, c. data worth assessment, which relates to FP, and d. decision support \citep{R21}.

In global sa, where, as one may recall, inputs are probabilistic in nature, one distinguishes six classes of sa methods \citep{BP16}. The sensitivity indices discussed and applied in this work belong to the area of variance-based sensitivity measures. Variance-based sensitivity measures are obtained by determining the expected reduction in output variance due to knowing input i with certainty \citep{BP16}. These methods are based on the classical formula for the law of total variance
\begin{equation}
V[Y]= V[E[Y \mid X_J]] + E[V[Y \mid X_J]]
\end{equation}

\noindent assuming that $f(X) < \infty$, where the $V$ operator stands for the unconditional variance of $Y, V[\cdot \mid \cdot]$ for the conditional variance, and $E[\cdot \mid \cdot]$ denotes the conditional expectation. The above expression decomposes total variance into the explained and the unexplained component (citep!).

A variance-based sensitivity measure determines much output variance is attributable to each input $i$ \citep{BP16}. Variance-based global sa is not applicable to decision variables, since global sa attaches a distribution to input variables, implying that inputs are uncertain \citep{SNS16}. Recall that due to this uncertainty in the inputs $X_K$ the output $Y$ is uncertain as well. $V[Y]$ measures this uncertainty in the output, where $V[Y]$ is taken according to the joint distribution of $G_K$. In the variance of the model output, there are three parts of variances; the one caused by every input in isolation, the one that is caused by interaction effects among inputs, and one that is due to input dependence.

\citet{S93} introduced often used variance-based sensitivity indices that attribute the variance reduction to each subset $J \subset K$ using an ANOVA decomposition. To ensure uniqueness of the ANOVA decomposition employed by \citet{S53}, one needs to assume independence of model inputs \citep{GM17}. Sobol' indices were introduced as a subset importance measure \citep{SNS16}. For the purposes of sa we let the subsets be the single inputs, i.e. the subsets under considerations are singletons. The Sobol' indices are then defined as
\begin{align}
S_i &= V[Y]^{-1} V[E[Y \mid X_i]]\\
S_i^T &= V[Y]^{-1}(V[Y] - V[E[Y \mid {X_{- i}}]])
\label[equation]{total_sobol}
\end{align}

\noindent where $X_{- i}$ is the subset of input variables without input $i$. What follows does apply to models with independent inputs only. The sensitivity measure $S_i$ is called the first-order sensitivity index. $S_i$ represents the share of output variance reduced by the isolated effect of input $i$, excluding contributions in variance reduction by interactions between input $i$ and the remaining $(K-1)$ inputs. The subtrahend in the nominator of \cref{total_sobol} can be seen as the expected variance reduction when $X_i$ is fixed at a certain value, i.e. if we know $X_i$ with certainty \citep{SNS16}. $S_i^T$ is called the total sensitivity index. $S_i^T$ complements $S_i$ in the sense that it measures the total effect of $X_i$ in the output variance, including interaction effects \citep{SNS16}. It can be considered the expected remaining output variance, when all values of the inputs are known, except for the value attached to $X_i$ \citep{SNS16}. By normalising both measures by $V[Y]$, it is clear that the value s of $S_i$ and $S_i^T$ are in the interval $[0, 1]$, since the numerators in equations xy and yz are always smaller than the total variance \citep{GM17}. For the relationship between $S_i$ and $S_i^T$ the weak inequality $S_i\ \le\ S_i^T$ holds true, while equality only holds when there are no interaction effects between $X_i$ and $X_{- i}$. Note that the terms indices and effects are used interchangeably.

Under input independence, the Sobol' indices have a clear interpretation. If $S_i$ is large, then the corresponding input $X_i$ is an influential input with respect to the output variance reduction \citep{GM17}. In contrast, a small first-order effect $S_i$ does not imply that $X_i$ is an uninfluential input, if strong interaction effects are present \citep{GM17}. As $S_i^T$ measures the total variance contribution by $X_i$, one can infer from a small $S_i^T$ that $X_i$ is indeed not uninfluential \citep{GM17}. If so, $X_i$ could be fixed at a certain value without causing changes in the model output variance \citep{GM17}.

Evaluating $S_i$ and $S_i^T$ one can also learn something about the structure of the model one is analysing, always assuming that inputs are independent. The model structure can be determined by the sums $\sum_{i \in K} S_i$ and $\sum_{i \in K} S_i^T$ \citep{GM17}.If both sums are equal to one, interaction effects are negligible , since the model is additive in nature. The model is non-additive if $\sum_{i \in K} S_i < 1$ and $\sum_{i \in K} S_i^T > 1$ \citep{GM17}. Thus, it can be inferred that interaction effects cannot be ignored and play a role in the system under consideration.

Recall that the statements about the model structure and input importance do not hold if inputs are dependent. In the case of dependence, one cannot simply apply the ANOVA decomposition since it is no longer unique \citep{O14}. In the Rust model, there are two input variables, $RC$ and $\theta_{11}$. As one can see in \cref{model_setup}, there exists significant dependence between the inputs. Thus, Sobol' indices should not be used. Another variance-based sensitivity measure are the now popular Shapley effects, which can be used also in the context of dependence. They are derived in the following section.

\subsection{Shapley Effects for Sensitivity Analysis}

In the context of sa, \citet{SNS16} named Shapley values Shapley effects, which were first suggested as a variance-based sensitivity measure by \citet{O14}. According to \citet{PRB20} the Shapley effect is becoming more and more popular in sa.

When applied as a sensitivity measure, the interpretation of the Shapley value changes. $X_i$ is now interpreted as a model input instead of a player in a game. A coalition is now a subset of model inputs. \citet{O14} defines the function $\widetilde{val}$ as a function that assigns the explanatory power of this subset of inputs to this subset. That is, the function $\widetilde{val}$ assigns the conditional variance to a subset of inputs, $\widetilde{val}(J)=V[Y]^{-1}V[E[Y \mid X_J]]$ and $\widetilde{val}$ measures the reduction in $V[Y]$ due to the inputs in $J$ \citep{SNS16}. \citep{SNS16} show that the following alternative formulation of the value function results into the same Shapley effect.
\begin{equation}
val(J)=E[V[Y \mid X_{- J}]]
\end{equation}

\noindent $val(J)$ can be interpreted as the remaining variance of $Y$, given the values of the inputs in $- J$ are known. Both formulations satisfy the following two requirements
\begin{align}
val(\emptyset)& = 0\\
val(K)& = V[Y].
\end{align}

\noindent In words, the value of the empty input subset should be zero and the value of the set of all inputs should equal the entire output variance. In this work I consider the value functions normalised by $val(K)=V[Y] $ to get Shapley effects normalised to the interval $ [0,\ 1] $. The most appealing properties of Shapley effects are that they satisfy the following two conditions (compare to axioms 1 and 4).
\begin{align}
\sum_{i=1}^{k}\phi_i& = 1\\
\phi_i& \geq 0,\ \forall \ i=1, ..., k.
\end{align}

\noindent So, the Shapley effects calculated for a model sum up to one and each Shapley effect is weakly larger than zero. Hence, the Shapley effects are input importance measures in terms of the expected output variance reduction induced by $X_i$. The non-negativity condition ensures that Shapley effects are always clearly interpretable. Note that due to the uncertainty in the computation process there exists the possibility of Shapley effects turning out to be below zero. By increasing sample sizes, this phenomenon should be mitigated.

Shapley effects can be used to compare input importance, that is, inputs can be ranked according to their contribution to output variance reduction. Furthermore, the differences between values of Shapley effects can be interpreted.

Recall the three parts of model output variance as discussed in \cref{var_based_sa}: variance due to the isolated effect of an input (i.e. the main variance), due to interaction effects, and due to dependence among inputs. Shapley effects take all three into account. In this regard they differ from first-order and total Sobol' indices as defined in \cref{var_based_sa} \citep{O14}.

In comparison to Shapley effects one has to acknowledge that Sobol' indices can inform about the model structure as discussed in \cref{var_based_sa}. In addition, Sobol' indices address more sa settings than Shapley effects, if inputs are independent. Shapley effects can be applied to FF only, since they distribute the effect of interactions between inputs equally across all inputs contained in the current subset \citep{IP19}. FP cannot be precisely conducted by using Shapley effects, since one cannot distinguish between contributions of main variance and variance contributions due to due to interactions \citep{IP19}.

Shapley effects yield a single value for each input that serves as the sensitivity measure, as opposed to Sobol' indices, which yield the first-order and total effects., i.e. two measures per input. Especially when computing sensitivity indices for studies that evaluate scientific phenomena, having a single value per input that informs about the variance contribution is very useful \citep{SNS16}. Furthermore, when this measure considers main and interaction effects and can also handle input dependence, one has a very useful and versatile sensitivity measure.

Sobol' indices for dependent inputs have been proposed \citep{MTA15}. This strategy is based on the estimation of four sensitivity indices, instead of the two measures in the case of input independence, elevating the practical usefulness of Shapley effects even more. In case of input dependence, Sobol' indices require a complicated ANOVA decomposition, whereas Shapley effects do not rely on such variance decompositions \citep{IP19}. When applying Shapley effects, one would not even need to know whether input dependence or independence prevails.

Several algorithms exist for estimating Shapley effects. See the following section.

\subsection{Algorithm} \label{comp_alg}

The algorithm for the computation of Shapley effects needs to allow for conditional sampling of dependent inputs. Otherwise, some of the advantages of Shapley effects cannot be exploited. I use algorithm 1 from \citet{SNS16}, see algorithm 1 in this paper.

Shapley effects consider all input subsets. To iterate over all subsets, \citet{SNS16} restate the equation for Shapley effects to iterate over all permutations of model inputs. For instance, let $k=5$. Then, $K=\{1,\ 2,\ 3,\ 4,\ 5\}$. There exist $5!$ permutations, where one is $\pi'=(2,\ 4,\ 3,\ 5,\ 1)$. Let the set of all permutations of $K$ be denoted by $\Pi(K)$. Further, let $P_i(\pi)$ denote all inputs in permutation $\pi$, that come before input $i$. In the above example, if $i=3$, then $P_3(\pi')=\{2,\ 4\}$. Using the permutation representation, \citet{SNS16} state the marginal contribution due to input $i$ as $val(P_i(\pi) \cup\{i\}) - val(P_i(\pi))$. By considering all permutations of $K$, we can restate the equation for the Shapley effects for input $i$ as
\begin{equation}
\phi_i=\sum_{\pi \in \Pi(K)} (k!)^{-1} (val(P_i(\pi) \cup\{i\}) - val(P_i(\pi))).
\end{equation}

The algorithm of \citet{SNS16} exploits the fact that their permutation-based algorithm evades redundant model evaluations by going through the permutations beginning with the smallest subset. The contribution of the preceding subset is subtracted from the contribution of the current subset. The marginal contribution is then computed by writing
\begin{equation}
val(P_{\pi(j)}(\pi) \cup \{\pi(j)\}) - val(P_{\pi(j)}(\pi)),
\end{equation}

\noindent where $\pi(j)$ denotes the input at position $j$ in permutation $\pi$. In the above permutation $\pi'$, at position 2 we have input 4, i.e. $\pi'(2)=4$.

To see for which subsets the algorithm performs evaluations of $val$, consider the below example. For instance, following \citet{SNS16}, if $k=3$ and $\pi=(1, 3, 2)$, the algorithm computes
\begin{align*}
\Delta_1 &=val(\{1\})-val(\emptyset),\ set\ prevC=0, \\
\Delta_2 &=val(\{1,\ 3\})-prevC,\ set\ prevC=val(\{1,\ 3\}), \\
\Delta_3 &=val(\{1,\ 3,\ 2\})-prevC,\ set\ prevC=val(\{1,\ 3,\ 2\}).
\end{align*}

Algorithm 1 implements three MC simulations. There are $N_V$ MC samples of model inputs which are evaluated to get an estimate of $V[Y]$. Since the algorithm needs to handle dependent inputs, \citet{SNS16} implement dependent sampling by an inner and an outer MC simulation. To see why both MC simulations are needed, fix a permutation $\pi$. Then, sample $N_O$ outer samples, that are unconditionally drawn. Given this set of unconditionally drawn outer samples, draw $N_I$ inner samples conditionally on the outer samples. That is, for each outer sample, one has $N_I$ inner samples. Thus, for algorithm 1, the computational cost in terms of number of model evaluations is given by $N_V+m \cdot N_I \cdot N_O \cdot (k-1)$, where $m$ is the number of permutations considered. In the exact setting of algorithm 1, $m=\vert \Pi(K) \vert=k!$. If only a random subset of permutations should be considered, one sets $m<k!$. This could be useful to reduce the computational burden if the number of inputs is large.

\subsection{Shapley Effects for the Rust Model}

\citet{SNS16} recommend setting $m=\vert \Pi(K) \vert$, if computationally feasible. In order to reduce variance of the estimates of Shapley effects, \citet{SNS16} recommend choosing $N_I=3$, while setting $N_O$ as large as possible, given the constraints on the computational budget. Since the Rust model has two inputs only, the number of permutations to be consiedered is two and thus, very small. Hence, I set $m=2!=2$, $N_I=3$, $N_V=100$ and $N_O=10$. Thus, the computational cost of my choice of MC runs imply that 160 model evaluations are needed to estimate the Shapley effects. On my machine (Windows 10, i5 processor), one estimation run takes approximately 55 seconds. I run the estimation by using the implementation of the package \textit{econsa}, a Python package for sa \citep{OSE21}.

I estimate 200 Shapley effects for the inputs of the Rust model, $RC$ and $\theta$ by the setup as described in the preceding section. The distribution of the 200 replicates is visualised by the boxplots in figure xy. For further details on the estimated replicates, see table xy. The mean of $\phi_{RC}$ is 0.415624. For $\phi_{\theta_{11}}$ the mean value is 0.584376. The variance of the Shapley effects is 0.00135.
Both, the boxplots, and the mean values show that $\theta_{11}$ is the more important input in terms of contributions to output variance. The confidence intervals at the 95-percent level, are quite large, but they do not overlap, indicating that the implied input importance ranking is robust. The lower value of $\phi_{RC}$ shows that $RC$ has less an impact on the output variance than $\theta_{11}$. Since both $\phi_i$ are far from zero, it is not recommended to fix any one of them, since both influence the output variance significantly.
