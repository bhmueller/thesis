\section{From Shapley Values to Shapley Effects}

This section introduces Shapley effects for sa.

\subsection{Shapley Values in Economics}

The Shapley value is a concept from cooperative game theory, introduced by \cite{S53}. It is a value that informs about how to fairly distribute the profit of a product that was created by a team effort. Fairly here means, that any player receives as much of the gain as he or she contributed to the team effort. As individual contributions are not directly observed, \cite{S53} proposes the following value. The Shapley value ensures that every individual receives at least as much as if he or she had if acted independently \cite{IP19}.

Consider a game with $k$ players, where $K$ denotes the set of all players, the grand coalition. A coalition is a subset $J \subset K$. Let $\tilde J$ denote the complement set of $J$, i.e. $\tilde J = K \setminus{J}$. The corresponding game is defined by a coalition value function that maps each coalition $J$ to the value attained by this coalition, i.e. $val: 2^K \to \mathbb{R}_{\geq 0}$, where $2^K$ denotes the power set of $K$ (set of all subsets of $K$) \cite{SNS16}. It is generally assumed that $val(\emptyset) = 0$. The following definition is strongly inspired by the one found in \cite{PRB20}:

\begin{definition}

Given a coalition worth function $val$, the marginal contribution of player $i$ joining coalition $J$ is $mar(J, i)=val(J \cup \{i\}) - val(J)$. The Shapley value is then defined by

\begin{equation}
\phi_{i} (val) = \sum_{J \mid i \notin J} \frac{\vert J \vert ! (k - \vert J \vert - 1) ! }{ k !} (val(J \cup \{i\}) - val(J)) \n = \frac{1}{k} \sum_{J \mid i \notin J} \binom{k-1}{\vert J \vert}^{-1} mar(J, i).
\end{equation}

\end{definition}

In words, one considers every coalition of players and evaluates by how much the inclusion of player $i$ increases the overall profit of the coalition. This is achieved by subtracting the value generated by the members of coalition $J$ from the value created by the larger coalition that includes coalition $J$ and player $i$. In the context of a game, the marginal value for each coalition is divided among all members of this coalition, e.g. if the coalition has one member only, the entire marginal value is gained by this player. One can also interpret the Shapley value as the expected payoff from joining a coalition or from leaving the complementary coalition \cite{PRB20}.

\cite{S53} shows that $\phi_i$ satisfies the following four axioms and that the Shapley value is the only value that satisfies all four axioms at the same time. Note that Shapley values normalised by total profit are considered.

\begin{itemize}
    \item Pareto efficiency: $\sum_{i=1}^{k}\phi_i=1$.
	\item Symmetry: If $val(J \cup {i})=val(J \cup {j})\ \forall\ J \subseteq K \setminus{\{i, j\}}$, then $\phi_i = \phi_j$.
	\item Linearity: $\phi_i(val_1 + val_2)=\phi_i(val_1)+\phi_i(val_2)$.
	\item Null-player: If $\forall J,\ mar(J, i) = 0$ holds then $\phi_i(val) = 0$.
\end{itemize}

Pareto efficiency assures that nothing of the profit gained by the team is wasted. Normalised by the total profit, the Shapley value gives the share attained by each individual in the game. Symmetry results into all players who contributed the same to the common product, receiving the same payoff, reflecting the meritocratic principle. The null-player axiom ensures that players that do not contribute to the team effort at all, do not receive any share in the profits. Especially axioms 1 and 4 are desirable in the context of the Shapley value as a variance-based sm.

\subsection{Variance-based sa} \label{var_based_sa}

There are several purposes for which one can apply sa methods in general. Note, that in sa one refers to input variables by the term factors (\cite{R21}). In their textbook, \cite{STC04} (chapter 2) summarise the following four possible objectives of sa.

Factors Prioritisation (FP): In the FP setting, one determines on which inputs applied, uncertainty reduction results into the largest reduction in output uncertainty. SA determines the importance of an input variable. Applied to all inputs, one can derive a ranking of all inputs in order of importance. FP can guide research by prioritising inputs. Inputs are identified for which better experimental measurement can reduce output uncertainty the most, supposing that additional information costs the same for alle inputs.

Factors Fixing (FF): The FF setting allows one to determine the least influential inputs. These then can be fixed at a specific value without losing information in the model output. FF can also be seen as inputs screening. One would want to fix the least influential inputs to reduce dimensionality of the model and thus the complexity and computational burden.

Variance Cutting (VC): VC informs about which inputs to fix to arrive at a certain desired value of the output variance, under the condition that the smallest number of inputs are fixed. VC is most useful for risk assessment.

Factors Mapping (FM): If one cares about the input importance in certain regions of output values only, one can apply FM. First, one classifies output values into groups and only then employs the importance exercise. So, the inputs are determined that contribute the most to producing output values in a target region. E.g. one could be interested in a certain percentile of the output range. The term mapping stems from mapping the importance of the inputs to the categories of $Y$.

SA does address more fundamental objectives of mathematical modelling and the analysis of systems that can be achieved directly from the above settings and are summarised in \cite{R21}. By sa, one can achieve or conduct a. scientific discovery, i.e. identification of causal relationships, b. dimensionality reduction, i.e. determination of the least important inputs, which is achieved by FF, c. data worth assessment, which relates to FP, and d. decision support \cite{R21}.
In global sa, where, as one may recall, inputs are probabilistic in nature, one distinguishes six classes of sa methods \cite{BP16}. The sensitivity indices discussed and applied in this work belong to the area of variance-based sensitivity measures. Variance-based sensitivity measures are obtained by determining the expected reduction in output variance due to knowing input i with certainty \cite{BP16}. These methods are based on the classical formula for the law of total variance

\begin{equation}
V[Y]= V[E[Y \mid X_J]] + E[V[Y \mid X_J]]
\end{equation}

assuming that $f(X)\ <\ ∞$, where the $V$ operator stands for the unconditional variance of $Y, V[|]$ for the conditional variance, and $E[.|.]$ denotes the conditional expectation. The above expression decomposes total variance into the explained and the unexplained component (cite!).
A variance-based sensitivity measure determines much output variance is attributable to each input i \cite{BP16}. Variance-based global sa is not applicable to decision variables, since global sa attaches a distribution to input variables, implying that inputs are uncertain \cite{SNS16}. Recall that due to this uncertainty in the inputs $X_K$ the output $Y$ is uncertain as well. $V[Y]$ measures this uncertainty in the output, where $V[Y]$ is taken according to the joint distribution of $G_K$. In the variance of the model output, there are three parts of variances; the one caused by every input in isolation, the one that is caused by interaction effects among inputs, and one that is due to input dependence.
\cite{S53} introduced often used variance-based sensitivity indices that attribute the variance reduction to each subset $J \subset K$ using an ANOVA decomposition. To ensure uniqueness of the ANOVA decomposition employed by \cite{S53}, one needs to assume independence of model inputs \cite{GM17}. Sobol’ indices were introduced as a subset importance measure \cite{SNS16}. For the purposes of sa we let the subsets be the single inputs, i.e. the subsets under considerations are singletons. The Sobol’ indices are then defined as

\begin{equation}
S_i\ =\ V[Y]^{-1} V[E[Y \mid X_i]]
\end{equation}

\begin{equation} \label[equation]{total_sobol}
S_i^T\ =\ V[Y]^{-1}(V[Y] - V[E[Y \mid {X_{\tilde i}}]])
\end{equation}

where $X_{\tilde i}$ is the subset of input variables without input $i$. What follows does apply to models with independent inputs only. The sensitivity measure $S_i$ is called the first-order sensitivity index. $S_i$ represents the share of output variance reduced by the isolated effect of input $i$, excluding contributions in variance reduction by interactions between input $i$ and the remaining $(K-1)$ inputs. The subtrahend in the nominator of \cref{total_sobol} can be seen as the expected variance reduction when $X_i$ is fixed at a certain value, i.e. if we know $X_i$ with certainty \cite{SNS16}. $S_i^T$ is called the total sensitivity index. $S_i^T$ complements $S_i$ in the sense that it measures the total effect of $X_i$ in the output variance, including interaction effects \cite{SNS16}. It can be considered the expected remaining output variance, when all values of the inputs are known, except for the value attached to $X_i$ \cite{SNS16}. By normalising both measures by $V[Y]$, it is clear that the value s of $S_i$ and $S_i^T$ are in the interval $[0, 1]$, since the numerators in equations xy and yz are always smaller than the total variance \cite{GM17}. For the relationship between $S_i$ and $S_i^T$ the weak inequality $S_i\ \le\ S_i^T$ holds true, while equality only holds when there are no interaction effects between $X_i$ and $X_{\tilde i}$. Note that the terms indices and effects are used interchangeably.
Under input independence, the Sobol’ indices have a clear interpretation. If $S_i$ is large, then the corresponding input $X_i$ is an influential input with respect to the output variance reduction \cite{GM17}. In contrast, a small first-order effect $S_i$ does not imply that $X_i$ is an uninfluential input, if strong interaction effects are present \cite{GM17}. As $S_i^T$ measures the total variance contribution by $X_i$, one can infer from a small $S_i^T$ that $X_i$ is indeed not uninfluential \cite{GM17}. If so, $X_i$ could be fixed at a certain value without causing changes in the model output variance \cite{GM17}.
Evaluating $S_i$ and $S_i^T$ one can also learn something about the structure of the model one is analysing, always assuming that inputs are independent. The model structure can be determined by the sums $\sum_{i\ \in\ K}\ S_i$ and $\sum_{i\ \in\ K}\ S_i^T$ \cite{GM17}.If both sums are equal to one, interaction effects are negligible , since the model is additive in nature. The model is non-additive if $\sum_{i\ \in\ K}\ S_i\ <\ 1$ and $\sum_{i\ \in\ K}\ S_i^T\ >\ 1$ \cite{GM17}. Thus, it can be inferred that interaction effects cannot be ignored and play a role in the system under consideration.
Recall that the statements about the model structure and input importance do not hold if inputs are dependent. In the case of dependence, one cannot simply apply the ANOVA decomposition since it is no longer unique \cite{O14}. In the Rust model, there are two input variables, $RC$ and $\theta_{11}$. As one can see in \cref{model_setup}, there exists significant dependence between the inputs. Thus, Sobol’ indices should not be used. Another variance-based sensitivity measure are the now popular Shapley effects, which can be used also in the context of dependence. They are derived in the following section.

\subsection{Shapley Effects for sa}

In the context of sa, \cite{SNS16} named Shapley values Shapley effects, which were first suggested as a variance-based sensitivity measure by \cite{O14}. According to \cite{PRB20} the Shapley effect is becoming more and more popular in sa.
When applied as a sensitivity measure, the interpretation of the Shapley value changes. $X_i$ is now interpreted as a model input instead of a player in a game. A coalition is now a subset of model inputs. \cite{O14} defines the function $val$ as a function that assigns the explanatory power of this subset of inputs to this subset. That is, the function $val$ assigns the conditional variance to a subset of inputs, $\tilde{val}(J)=V[Y]^{-1}V[E[Y \mid X_J]]$ and $\tilde{val}$ measures the reduction in $V[Y]$ due to the inputs in $J$ \cite{SNS16}. \cite{SNS16} show that the following alternative formulation of the value function results into the same Shapley effect.

\begin{equation}
val(J)=E[V[Y\ \mid\ X_{\tilde J}]]
\end{equation}

$val(J)$ can be interpreted as the remaining variance of $Y$, given the values of the inputs in $\tilde J$ are known. Both formulations satisfy the following two requirements

\begin{equation}
val(\emptyset) = 0
\end{equation}

\begin{equation}
val(K)=V[Y].
\end{equation}

In words, the value of the empty input subset should be zero and the value of the set of all inputs should equal the entire output variance. In this work I consider the value functions normalised by $val(K)=V[Y] $ to get Shapley effects normalised to the interval $ [0, 1] $. The most appealing properties of Shapley effects are that they satisfy the following two conditions (compare to axioms 1).

\begin{equation}
\sum_{i=1}^{k}\phi_i=1
\end{equation}

\begin{equation}
phi_i\ \ \geq\ 0,\ \forall\ i=1,\ ...,\ k.
\end{equation}

So, the Shapley effects calculated for a model sum up to one and each Shapley effect is weakly larger than zero. Hence, the Shapley effects are input importance measures in terms of the expected output variance reduction induced by $X_i$. The non-negativity condition ensures that Shapley effects are always clearly interpretable. Note that due to the uncertainty in the computation process there exists the possibility of Shapley effects turning out to be below zero. By increasing sample sizes, this phenomenon should be mitigated.
Shapley effects can be used to compare input importance, that is, inputs can be ranked according to their contribution to output variance reduction. Furthermore, the differences between values of Shapley effects can be interpreted.
Recall the three parts of model output variance as discussed in \cref{var_based_sa}: variance due to the isolated effect of an input (i.e. the main variance), due to interaction effects, and due to dependence among inputs. Shapley effects take all three into account. In this regard they differ from first-order and total Sobol’ indices as defined in \cref{var_based_sa} \cite{O14}.
In comparison to Shapley effects one has to acknowledge that Sobol’ indices can inform about the model structure as discussed in \cref{var_based_sa}. In addition, Sobol’ indices address more sa settings than Shapley effects, if inputs are independent. Shapley effects can be applied to FF only, since they distribute the effect of interactions between inputs equally across all inputs contained in the current subset \cite{IP19}. FP cannot be precisely conducted by using Shapley effects, since one cannot distinguish between contributions of main variance and variance contributions due to due to interactions \cite{IP19}.
Shapley effects yield a single value for each input that serves as the sensitivity measure, as opposed to Sobol’ indices, which yield the first-order and total effects., i.e. two measures per input. Especially when computing sensitivity indices for studies that evaluate scientific phenomena, having a single value per input that informs about the variance contribution is very useful \cite{SNS16}. Furthermore, when this measure considers main and interaction effects and can also handle input dependence, one has a very useful and versatile sensitivity measure.
Sobol’ indices for dependent inputs have been proposed \cite{MTA15}. This strategy is based on the estimation of four sensitivity indices, instead of the two measures in the case of input independence, elevating the practical usefulness of Shapley effects even more. In case of input dependence, Sobol’ indices require a complicated ANOVA decomposition, whereas Shapley effects do not rely on such variance decompositions \cite{IP19}. When applying Shapley effects, one would not even need to know whether input dependence or independence prevails.
Several algorithms exist for estimating Shapley effects. See \cref{comp_alg}. The next section introduces the Rust model, the model I want to compute Shapley effects for.
